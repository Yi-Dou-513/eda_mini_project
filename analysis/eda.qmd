---
title: "Assignment3"
author: "Yi Dou"
format: 
  html:
    embed-resources: true
editor: visual
warning: false
message: false
bibliography: references.bib
---

## Exercise 03

```{r, message = FALSE}
library(tidyverse)
crime_data <- read_csv("data/Crime_Incidents_in_2025.csv")
crime_data <- crime_data |>
  select(CCN, REPORT_DAT, START_DATE, END_DATE, BLOCK, 
         OFFENSE, METHOD, SHIFT, LATITUDE, LONGITUDE) 
crime_data |>
  glimpse()
```

This project uses administrative crime data from Washington, DC (@dc_crime_2025). The time period represented by the data is from 2025-01-01 to 2026-01-01(UTC). The data were collected through administrative records from the Metropolitan Police Department based on reported crime incidents. Each observation represents a single reported crime incident and can be uniquely identified using the Crime Control Number (CCN). The data set is administrative and cross-sectional data including numeric, categorical, timeseries and geospatial variables. It does not include weights.

## Exercise 04

```{r}
sapply(crime_data, class)
crime_data <- crime_data |>
  mutate(
    report_datetime = ymd_hms(REPORT_DAT, tz = "UTC"),
    start_datetime  = ymd_hms(START_DATE,  tz = "UTC"),
    end_datetime    = ymd_hms(END_DATE,    tz = "UTC")
  )
str(crime_data$report_datetime)
```

I examined the variable class of the data set. While other variables aligned with the data documentation, the three datetime variables were stored as strings. Luckily, they had all the information I needed, including date, time, and time zone. So I used the function in the lubridate package to transform them into datetime format.

## Exercise 05

```{r}
crime_data <- crime_data |>
  mutate(crime_duration = as.numeric(end_datetime - start_datetime, units = "hours"),
         reporting_delay = as.numeric(report_datetime - start_datetime, units = "hours"),
         month = month(start_datetime, label = TRUE, locale = "English"),
         quadrant = str_extract(BLOCK, "\\b(NE|NW|SE|SW)\\b"))
crime_data |>
  count(OFFENSE)
crime_data |>
  count(METHOD)
crime_data |>
  group_by(month) |>
  summarise(monthly_crime_frequency = n())
crime_data |>
  group_by(month) |>
  summarise(monthly_crime_frequency = n()) |>
  summarise(monthly_crime_frequency_mean = mean(monthly_crime_frequency),
            monthly_crime_frequency_median = median(monthly_crime_frequency),
            monthly_crime_frequency_max = max(monthly_crime_frequency),
            monthly_crime_frequency_min = min(monthly_crime_frequency))
crime_data |>
  count(SHIFT) |>
  mutate(proportion = n / sum(n)) |>
  select(SHIFT, proportion) 
crime_data |>
  summarise(crime_duration_mean = mean(crime_duration, na.rm = TRUE),
            crime_duration_sd = sd(crime_duration, na.rm = TRUE))
crime_data |>
  summarise(reporting_delay_mean = mean(reporting_delay, na.rm = TRUE),
            reporting_delay_sd = sd(reporting_delay, na.rm = TRUE))
crime_data |>
  group_by(quadrant) |>
  summarise(crime_duration_median = median(crime_duration, na.rm = TRUE),
            crime_duration_q75 = quantile(crime_duration, 0.75, na.rm = TRUE),
            reporting_delay_median = median(reporting_delay, na.rm = TRUE),
            reporting_delay_q75 = quantile(reporting_delay, 0.75, na.rm = TRUE))
  
```

Here I listed some summary statistics of this data set, including 8 different kinds of statistics(frequency, proportion, mean, median, max, min, sd and quantile) and 3 continuous variables(crime frequency, crime duration and reporting delay) based on 5 grouping variables(OFFENSE, METHOD, month, SHIFT and quadrant). I think these summary statistics are sufficient for understanding the univariate distributions in the data, as they show the concentration and dispersion of the key variable across different grouping variables.

## Exercise 06

```{r}
monthly_counts <- crime_data |>
  filter(!is.na(quadrant)) |>
  count(quadrant, month)
monthly_mean <- monthly_counts |>
  group_by(month) |>
  summarise(quadrant_mean = mean(n))

ggplot(data = monthly_counts,
       mapping = aes(x = month, y = n, fill = quadrant)) +
  geom_col(position = "dodge") +
  geom_line(data = monthly_mean,
            aes(x = month, y = quadrant_mean, group = 1),
            inherit.aes = FALSE,
            color = "black",
            linewidth = 0.75) +
  scale_fill_manual(values = c(NE = "orange", NW = "red", 
                               SE = "purple",  SW = "blue" )) +
  labs(x = "Month",
       y = "Crime Frequency",
       fill = "Quadrant",
       title = paste0("Monthly Crime Counts by Geographic Quadrant",
                      "in Washington, DC (2025)") ,
       subtitle = paste0("Bars show monthly crime incidents by quadrant; ",
                         "the line indicates the average across quadrants") ) +
  theme_minimal()

```

```{r}
crime_data |>
  slice_sample(n = 200) |>
  ggplot(aes(x = LONGITUDE, y = LATITUDE)) +
  geom_point(na.rm = TRUE, color = "blue", alpha = 0.4) +
  geom_vline(xintercept = -77.0091, linetype = "dashed", color = "black") +
  geom_hline(yintercept = 38.8899, linetype = "dashed", color = "black") +
  scale_x_continuous(
    name = "Longitude (째W)",
    labels = function(x) paste0(abs(round(x, 2)), "째W")
  ) +
  scale_y_continuous(
    name = "Latitude (째N)",
    labels = function(y) paste0(round(y, 2), "째N")
  ) +
  labs(title = paste0("Spatial Distribution of Crime Incidents",
                      "in Washington, DC (Sample of 200)"),
       subtitle = paste0("Points show sampled crime locations;",
                         "dashed lines indicate quadrant boundaries",
                         "relative to the U.S. Capitol")) +
  theme_minimal() 

```

## Exercise 07

Here are some additional analysis I would perform on the data:

-   Merging it with some data sets which contains demographic data of Washington, DC to understand the distribution of crimes more thoroughly
-   Finding the determinants of reporting delay by running some machine learning models
-   Creating some real geospatial visualizations which is more readable than the second figure based on the longitude and latitude data
